{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5471e151-fc94-4bcc-8222-4a1f7d1b19cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68768, 1024])\n",
      "torch.Size([45630, 1024])\n",
      "(676946, 2)\n",
      "(283440, 2)\n",
      "   userID  itemID\n",
      "0       0       0\n",
      "1       0       1\n",
      "2       0       2\n",
      "3       1       3\n",
      "4       2       4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_path = \"../../data/jeehoshin/allrecipe_dataset/\"\n",
    "\n",
    "user_pt = torch.load(data_path + \"user_embedding.pt\")\n",
    "item_pt = torch.load(data_path + \"item_embedding.pt\")\n",
    "\n",
    "user_embedding = nn.Embedding(user_pt['weight'].shape[0], user_pt['weight'].shape[1]).to(device)\n",
    "item_embedding = nn.Embedding(item_pt['weight'].shape[0], item_pt['weight'].shape[1]).to(device)\n",
    "user_embedding.load_state_dict(user_pt)\n",
    "item_embedding.load_state_dict(item_pt)\n",
    "\n",
    "print(user_embedding.weight.shape)\n",
    "print(item_embedding.weight.shape)\n",
    "\n",
    "df = pd.read_csv(data_path + \"allrecipe.inter\", sep='\\t')\n",
    "train_interaction = df[df['x_label'] == 0][['userID', 'itemID']]\n",
    "test_interaction = df[df['x_label'] == 2][['userID', 'itemID']]\n",
    "print(train_interaction.shape)\n",
    "print(test_interaction.shape)\n",
    "\n",
    "print(train_interaction.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5649bcd3-3790-4255-acec-bfc01d9a505c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68768 45630\n",
      "A done (fast)\n",
      "torch.Size([114398, 114398])\n",
      "torch.Size([68768, 45630])\n",
      "torch.Size([68768, 45630])\n",
      "torch.Size([68768, 45630])\n",
      "torch.Size([68768, 45630])\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "n_users = user_embedding.weight.shape[0]\n",
    "n_items = item_embedding.weight.shape[0]\n",
    "max_layers = 4\n",
    "\n",
    "print(n_users, n_items)\n",
    "\n",
    "# Extract user and item IDs as NumPy arrays\n",
    "user_ids = train_interaction['userID'].to_numpy()\n",
    "item_ids = train_interaction['itemID'].to_numpy()\n",
    "\n",
    "# Create interaction edges (user→item and item→user)\n",
    "row = np.concatenate([user_ids, item_ids + n_users])\n",
    "col = np.concatenate([item_ids + n_users, user_ids])\n",
    "data = np.ones(len(row), dtype=np.float32)\n",
    "\n",
    "# Create symmetric adjacency matrix A as COO\n",
    "A = sp.coo_matrix((data, (row, col)), shape=(n_users + n_items, n_users + n_items))\n",
    "\n",
    "print('A done (fast)')\n",
    "\n",
    "# Compute normalized adjacency matrix L = D^(-0.5) * A * D^(-0.5)\n",
    "sumArr = np.array(A.sum(axis=1)).flatten() + 1e-7\n",
    "diag = np.power(sumArr, -0.5)\n",
    "D = sp.diags(diag)\n",
    "L = D @ A @ D  # Matrix multiplication\n",
    "\n",
    "# Convert to PyTorch sparse tensor\n",
    "L = sp.coo_matrix(L)\n",
    "indices = torch.from_numpy(np.vstack((L.row, L.col)).astype(np.int64))\n",
    "values = torch.from_numpy(L.data.astype(np.float32))\n",
    "SparseL = torch.sparse_coo_tensor(indices, values, size=torch.Size(L.shape)).to(device)\n",
    "\n",
    "print(SparseL.shape)\n",
    "\n",
    "all_embeddings = torch.cat([user_embedding.weight, item_embedding.weight], 0).to(device)\n",
    "embeddings_list = [all_embeddings]\n",
    "scores = []\n",
    "\n",
    "user_all_embeddings = all_embeddings[:n_users, :]\n",
    "item_all_embeddings = all_embeddings[n_users:, :]\n",
    "score = torch.matmul(user_all_embeddings, item_all_embeddings.T)\n",
    "scores.append(score.detach().cpu())\n",
    "\n",
    "for i in range(max_layers):\n",
    "    all_embeddings = torch.sparse.mm(SparseL, all_embeddings)\n",
    "    embeddings_list.append(all_embeddings)\n",
    "    lightgcn_all_embeddings = torch.stack(embeddings_list, dim=1)\n",
    "    lightgcn_all_embeddings = torch.mean(lightgcn_all_embeddings, dim=1)\n",
    "    user_all_embeddings = lightgcn_all_embeddings[:n_users, :]\n",
    "    item_all_embeddings = lightgcn_all_embeddings[n_users:, :]\n",
    "    score = torch.matmul(user_all_embeddings, item_all_embeddings.T).to(device)\n",
    "    scores.append(score.detach().cpu())\n",
    "    print(score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45adf85-7feb-4db4-879b-420861590347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Score 0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████| 68768/68768 [00:04<00:00, 15513.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10: 0.0008\n",
      "Recall@20: 0.0014\n",
      "NDCG@10: 0.0005\n",
      "NDCG@20: 0.0007\n",
      "\n",
      "<Score 1>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████| 68768/68768 [00:04<00:00, 16337.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10: 0.0232\n",
      "Recall@20: 0.0442\n",
      "NDCG@10: 0.0148\n",
      "NDCG@20: 0.0211\n",
      "\n",
      "<Score 2>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████| 68768/68768 [00:04<00:00, 15558.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10: 0.0209\n",
      "Recall@20: 0.0441\n",
      "NDCG@10: 0.0140\n",
      "NDCG@20: 0.0208\n",
      "\n",
      "<Score 3>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████| 68768/68768 [00:04<00:00, 15270.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10: 0.0202\n",
      "Recall@20: 0.0442\n",
      "NDCG@10: 0.0136\n",
      "NDCG@20: 0.0206\n",
      "\n",
      "<Score 4>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████| 68768/68768 [00:04<00:00, 15686.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10: 0.0201\n",
      "Recall@20: 0.0442\n",
      "NDCG@10: 0.0135\n",
      "NDCG@20: 0.0204\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_ground_truth_matrix(df, num_users, num_items):\n",
    "    \"\"\"Return a sparse matrix of shape [num_users, num_items] where entries are 1 if interacted.\"\"\"\n",
    "    user_ids = df['userID'].to_numpy()\n",
    "    item_ids = df['itemID'].to_numpy()\n",
    "    gt_matrix = sp.csr_matrix((np.ones_like(user_ids), (user_ids, item_ids)),\n",
    "                              shape=(num_users, num_items), dtype=np.int8)\n",
    "    return gt_matrix\n",
    "\n",
    "def evaluate_topk(score, gt_matrix, ks=[10, 20]):\n",
    "    \"\"\"Evaluate Recall@k and NDCG@k for all users.\"\"\"\n",
    "    topk = max(ks)\n",
    "    score = score.detach().cpu()\n",
    "\n",
    "    # Get top-K predictions for all users at once\n",
    "    topk_items = torch.topk(score, k=topk, dim=1).indices.numpy()  # shape: [num_users, topk]\n",
    "    gt_matrix = gt_matrix.tolil()  # fast row access\n",
    "\n",
    "    recalls = {k: [] for k in ks}\n",
    "    ndcgs = {k: [] for k in ks}\n",
    "\n",
    "    for user_id in tqdm(range(score.shape[0]), desc=\"Evaluating\"):\n",
    "        true_items = set(gt_matrix.rows[user_id])\n",
    "        if not true_items:\n",
    "            continue  # skip users without ground truth\n",
    "\n",
    "        preds = topk_items[user_id]\n",
    "\n",
    "        for k in ks:\n",
    "            pred_k = preds[:k]\n",
    "            hits = [1 if item in true_items else 0 for item in pred_k]\n",
    "            recall = sum(hits) / len(true_items)\n",
    "\n",
    "            dcg = sum([hit / np.log2(i + 2) for i, hit in enumerate(hits)])\n",
    "            idcg = sum([1.0 / np.log2(i + 2) for i in range(min(len(true_items), k))])\n",
    "            ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "            recalls[k].append(recall)\n",
    "            ndcgs[k].append(ndcg)\n",
    "\n",
    "    return {\n",
    "        f\"Recall@{k}\": np.mean(recalls[k]) for k in ks\n",
    "    } | {\n",
    "        f\"NDCG@{k}\": np.mean(ndcgs[k]) for k in ks\n",
    "    }\n",
    "\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Precompute ground truth matrix once\n",
    "gt_matrix = get_ground_truth_matrix(test_interaction, n_users, n_items)\n",
    "\n",
    "for idx, score in enumerate(scores):\n",
    "    print(f\"<Score {idx}>\")\n",
    "    metrics = evaluate_topk(score, gt_matrix, ks=[10, 20])\n",
    "    for name, val in metrics.items():\n",
    "        print(f\"{name}: {val:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccbf5cd4-e138-475a-ba22-aa1e9b97cad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68768, 45630)\n",
      "(68768, 45630)\n",
      "<Compressed Sparse Row sparse matrix of dtype 'bool'\n",
      "\twith 0 stored elements and shape (68768, 45630)>\n",
      "Number of overlapping interactions: 0\n"
     ]
    }
   ],
   "source": [
    "# check overlap in train / test interaction\n",
    "gt_train = get_ground_truth_matrix(train_interaction, n_users, n_items)\n",
    "gt_test = get_ground_truth_matrix(test_interaction, n_users, n_items)\n",
    "\n",
    "print(gt_train.shape)\n",
    "print(gt_test.shape)\n",
    "\n",
    "summation = gt_train + gt_test\n",
    "\n",
    "overlap_matrix = summation > 1  # element-wise comparison, returns sparse boolean matrix\n",
    "print(overlap_matrix)\n",
    "n_overlaps = overlap_matrix.nnz  # number of non-zero entries (i.e., overlaps)\n",
    "\n",
    "print(f\"Number of overlapping interactions: {n_overlaps}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
