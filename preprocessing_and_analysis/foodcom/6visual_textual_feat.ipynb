{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01522931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "11.8\n",
      "cuda\n",
      "True\n",
      "0\n",
      "<function get_device_name at 0x7fe1836485e0>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff0317c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29943\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>recipe_id</th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2999</td>\n",
       "      <td>3567</td>\n",
       "      <td>2000-10-23</td>\n",
       "      <td>5</td>\n",
       "      <td>I have made this pie instead of plain ol' pump...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2178</td>\n",
       "      <td>3704</td>\n",
       "      <td>2000-10-30</td>\n",
       "      <td>3</td>\n",
       "      <td>Careful not to cook it too long... you want th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2178</td>\n",
       "      <td>4366</td>\n",
       "      <td>2000-11-04</td>\n",
       "      <td>5</td>\n",
       "      <td>if you like oysters, this is a great alternati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5523</td>\n",
       "      <td>7695</td>\n",
       "      <td>2001-02-01</td>\n",
       "      <td>1</td>\n",
       "      <td>I agree.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42189</td>\n",
       "      <td>4460</td>\n",
       "      <td>2001-02-12</td>\n",
       "      <td>5</td>\n",
       "      <td>I have had this before. It has really good fla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428496</th>\n",
       "      <td>2001513060</td>\n",
       "      <td>367414</td>\n",
       "      <td>2018-12-17</td>\n",
       "      <td>1</td>\n",
       "      <td>maybe I did something wrong , but I thought th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428497</th>\n",
       "      <td>2001513060</td>\n",
       "      <td>192495</td>\n",
       "      <td>2018-12-17</td>\n",
       "      <td>5</td>\n",
       "      <td>This is a keeper. Delicious Soup that both my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428498</th>\n",
       "      <td>454804</td>\n",
       "      <td>20713</td>\n",
       "      <td>2018-12-17</td>\n",
       "      <td>0</td>\n",
       "      <td>Made this as gifts. Did 6, quart jars plus had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428499</th>\n",
       "      <td>1290903</td>\n",
       "      <td>131607</td>\n",
       "      <td>2018-12-18</td>\n",
       "      <td>5</td>\n",
       "      <td>This is a great recipe for a nice thin crispy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428500</th>\n",
       "      <td>226867</td>\n",
       "      <td>363072</td>\n",
       "      <td>2018-12-18</td>\n",
       "      <td>5</td>\n",
       "      <td>Leaping Lizards, Lori! These are wonderful! I ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428501 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           user_id  recipe_id        date  rating  \\\n",
       "0             2999       3567  2000-10-23       5   \n",
       "1             2178       3704  2000-10-30       3   \n",
       "2             2178       4366  2000-11-04       5   \n",
       "3             5523       7695  2001-02-01       1   \n",
       "4            42189       4460  2001-02-12       5   \n",
       "...            ...        ...         ...     ...   \n",
       "428496  2001513060     367414  2018-12-17       1   \n",
       "428497  2001513060     192495  2018-12-17       5   \n",
       "428498      454804      20713  2018-12-17       0   \n",
       "428499     1290903     131607  2018-12-18       5   \n",
       "428500      226867     363072  2018-12-18       5   \n",
       "\n",
       "                                                   review  \n",
       "0       I have made this pie instead of plain ol' pump...  \n",
       "1       Careful not to cook it too long... you want th...  \n",
       "2       if you like oysters, this is a great alternati...  \n",
       "3                                                I agree.  \n",
       "4       I have had this before. It has really good fla...  \n",
       "...                                                   ...  \n",
       "428496  maybe I did something wrong , but I thought th...  \n",
       "428497  This is a keeper. Delicious Soup that both my ...  \n",
       "428498  Made this as gifts. Did 6, quart jars plus had...  \n",
       "428499  This is a great recipe for a nice thin crispy ...  \n",
       "428500  Leaping Lizards, Lori! These are wonderful! I ...  \n",
       "\n",
       "[428501 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "data_path = '../../data/jeehoshin/foodcom_dataset/'\n",
    "\n",
    "raw_interaction = pd.read_csv(data_path + 'RAW_interactions.csv')\n",
    "pp_recipe = pd.read_csv(data_path + 'PP_recipes.csv')\n",
    "raw_recipe = pd.read_csv(data_path + 'RAW_recipes.csv')\n",
    "\n",
    "records_removed = True\n",
    "df = raw_interaction[raw_interaction['recipe_id'].isin(pp_recipe['id'].tolist())].reset_index(drop=True)\n",
    "\n",
    "while records_removed:\n",
    "    user_counts = df['user_id'].value_counts()\n",
    "    item_counts = df['recipe_id'].value_counts()\n",
    "\n",
    "    valid_users = user_counts[user_counts >= 5].index\n",
    "    valid_items = item_counts[item_counts >= 5].index\n",
    "\n",
    "    filtered_df = df[df['user_id'].isin(valid_users) & df['recipe_id'].isin(valid_items)].reset_index(drop=True)\n",
    "\n",
    "    if len(filtered_df) == len(df):\n",
    "        records_removed = False\n",
    "    else:\n",
    "        records_removed = True\n",
    "\n",
    "    df = filtered_df\n",
    "    \n",
    "core_inter = df\n",
    "sort_filter_ui = core_inter.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "file_names = os.listdir(data_path + 'images/')\n",
    "\n",
    "finish_image = [int(os.path.splitext(file_name)[0]) for file_name in file_names]\n",
    "sort_filter_ui = sort_filter_ui[sort_filter_ui['recipe_id'].isin(finish_image)].reset_index(drop=True)\n",
    "print(len(finish_image))\n",
    "sort_filter_ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc077632",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"# of recipes with images : {len(file_names)}\")\n",
    "\n",
    "users = sort_filter_ui['user_id'].unique().tolist()\n",
    "items = sort_filter_ui['recipe_id'].unique().tolist()\n",
    "print(f\"number of users: {len(users)}, number of items: {len(items)}\")\n",
    "\n",
    "train_ = sort_filter_ui[:int(0.6*len(sort_filter_ui))]\n",
    "valid_ = sort_filter_ui[int(0.6*len(sort_filter_ui)):int(0.7*len(sort_filter_ui))]\n",
    "test_ = sort_filter_ui[int(0.7*len(sort_filter_ui)):]\n",
    "\n",
    "u_tr = set(train_['user_id'].tolist())\n",
    "u_va = set(valid_['user_id'].tolist())\n",
    "u_te = set(test_['user_id'].tolist())\n",
    "u_total = u_tr & u_te\n",
    "\n",
    "filter_u_tr = train_[train_['user_id'].isin(u_total)].reset_index(drop=True)\n",
    "filter_u_te = test_[test_['user_id'].isin(u_total)].reset_index(drop=True)\n",
    "filter_u_va = valid_[valid_['user_id'].isin(u_total)].reset_index(drop=True)\n",
    "print(f\"train interaction count : {len(filter_u_tr)}, valid interaction count : {len(filter_u_va)}, test interaction count : {len(filter_u_te)}\")\n",
    "\n",
    "u_train = set(filter_u_tr['user_id'].tolist())\n",
    "u_test = set(filter_u_te['user_id'].tolist())\n",
    "u_valid = set(filter_u_va['user_id'].tolist())\n",
    "print(f\"train user count : {len(u_train)}, valid user count : {len(u_valid)}, test user count : {len(u_test)}\")\n",
    "\n",
    "i_tr = set(filter_u_tr['recipe_id'].tolist())\n",
    "i_te = set(filter_u_te['recipe_id'].tolist())\n",
    "i_va = set(filter_u_va['recipe_id'].tolist())\n",
    "print(f\"train item count : {len(i_tr)}, valid item count : {len(i_va)}, test item count : {len(i_te)}\")\n",
    "\n",
    "i_total = i_tr|i_va|i_te\n",
    "print(f\"number of users to use : {len(u_total)}\")\n",
    "print(f\"number of items to use : {len(i_total)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639b6652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "train_interaction = filter_u_tr\n",
    "test_interaction = filter_u_te\n",
    "valid_interaction = filter_u_va\n",
    "print(f\"train interaction count : {len(train_interaction)}, valid interaction count : {len(valid_interaction)}, test interaction count : {len(test_interaction)}\")\n",
    "print(train_interaction.columns)\n",
    "print(test_interaction.columns)\n",
    "print(valid_interaction.columns)\n",
    "\n",
    "recipe_processed = raw_recipe[raw_recipe['id'].isin(i_total)].reset_index(drop=True)\n",
    "print(f\"number of recipes to use : {len(recipe_processed)}\")\n",
    "print(recipe_processed.columns)\n",
    "\n",
    "# calories (#), total fat (PDV), sugar (PDV) , sodium (PDV) , protein (PDV) , saturated fat (PDV), carbohydrates (PDV)\n",
    "def process_row_detailed(row):\n",
    "    nutrition_types = ['calories', 'fat', 'sugar', 'sodium', 'protein', 'saturated_fat', 'carbohydrates']\n",
    "    nutrition = ast.literal_eval(row['nutrition'])\n",
    "    cooking_steps = ast.literal_eval(row['steps'])\n",
    "\n",
    "    parts = []\n",
    "    for n_type, value in zip(nutrition_types, nutrition):\n",
    "        if int(value) == value:\n",
    "            value = int(value)\n",
    "        if n_type == 'calories':\n",
    "            parts.append(f\"calories: {value} kcal\")\n",
    "        else:\n",
    "            parts.append(f\"{n_type}: {value}% of Daily Value\")\n",
    "    \n",
    "    nutrition_summary = \", \".join(parts)\n",
    "\n",
    "    parts2 = []\n",
    "    for idx, step in enumerate(cooking_steps):\n",
    "        parts2.append(f\"{idx+1}. {step}\")\n",
    "    cooking_steps_summary = \"\\n\".join(parts2)\n",
    "\n",
    "    ingredients = \", \".join(ast.literal_eval(row['ingredients']))\n",
    "    \n",
    "    return pd.Series({\n",
    "        'id': row['id'],\n",
    "        'name': row['name'],\n",
    "        'ingredients': ingredients,\n",
    "        'nutrition': nutrition_summary,\n",
    "        'steps' : cooking_steps_summary\n",
    "    })\n",
    "\n",
    "drop_columns_detailed = ['minutes', 'contributor_id', 'submitted', 'tags', 'n_steps',  'description', 'n_ingredients']\n",
    "recipe_drop_detailed = recipe_processed.drop(columns=drop_columns_detailed)\n",
    "\n",
    "detailed_summary_recipe = recipe_drop_detailed.apply(process_row_detailed, axis=1)\n",
    "print(detailed_summary_recipe.columns)\n",
    "print(detailed_summary_recipe.head(3))\n",
    "\n",
    "# Save the preprocessed recipe textual metadata for user embedding extraction\n",
    "detailed_summary_recipe.to_csv(data_path + 'recipe_textual_metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72fe1cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29905\n",
      "      id                                               name  \\\n",
      "0  63986                   chicken lickin  good  pork chops   \n",
      "1  43026                                     chile rellenos   \n",
      "2  54100                           grilled  venison burgers   \n",
      "3  25775  how i got my family to eat spinach  spinach ca...   \n",
      "4  90921         i stole the idea from mirj  sesame noodles   \n",
      "\n",
      "                                         ingredients  \\\n",
      "0  lean pork chops, flour, salt, dry mustard, gar...   \n",
      "1  egg roll wrap, whole green chilies, cheese, co...   \n",
      "2  ground venison, egg substitute, non-fat powder...   \n",
      "3  frozen chopped spinach, egg, salt, black peppe...   \n",
      "4  angel hair pasta, toasted sesame oil, soy sauc...   \n",
      "\n",
      "                                           nutrition  \\\n",
      "0  calories: 105.7 kcal, fat: 8% of Daily Value, ...   \n",
      "1  calories: 94 kcal, fat: 10% of Daily Value, su...   \n",
      "2  calories: 190.9 kcal, fat: 10% of Daily Value,...   \n",
      "3  calories: 166.1 kcal, fat: 16% of Daily Value,...   \n",
      "4  calories: 783.4 kcal, fat: 46% of Daily Value,...   \n",
      "\n",
      "                                               steps  \n",
      "0  1. dredge pork chops in mixture of flour , sal...  \n",
      "1  1. drain green chiles\\n2. sprinkle cornstarch ...  \n",
      "2  1. in bowl , mix dry ingredients\\n2. add venis...  \n",
      "3  1. preheat oven to 350 degrees\\n2. place spina...  \n",
      "4  1. in a large pot , cook your angel hair pasta...  \n"
     ]
    }
   ],
   "source": [
    "print(len(detailed_summary_recipe))\n",
    "dd = pd.read_csv(data_path + 'recipe_textual_metadata.csv')\n",
    "print(dd.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c79707a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "visual_encoding: 100%|███████████████| 29905/29905 [10:14<00:00, 48.64it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "205fddd8cadd4bdc90d7f9745c78c7f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/935 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   recipe_id                                     visual_feature\n",
      "0      63986  [0.0, 3.1429787, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "1      43026  [0.0, 0.862355, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "2      54100  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "3      25775  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "4      90921  [0.0, 6.0908422, 0.0, 0.0, 0.1583686, 0.0, 0.0...\n",
      "   recipe_id                                    textual_feature\n",
      "0      63986  [-0.10267617, -0.016740197, 0.017686494, 0.004...\n",
      "1      43026  [-0.06881522, -0.017232738, -0.047181953, 0.05...\n",
      "2      54100  [-0.056857362, -0.024174849, 0.0030214894, -0....\n",
      "3      25775  [-0.027166935, -0.017478112, -0.07573529, -0.0...\n",
      "4      90921  [-0.0876921, -0.03924904, 0.027581185, 0.03766...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "recipe_ids = list(detailed_summary_recipe['id'].unique())\n",
    "\n",
    "# =========================\n",
    "# [Visual Feature] CaffeNet\n",
    "# =========================\n",
    "# CaffeNet ≈ AlexNet. Use torchvision's alexnet and remove the final 4096->1000 layer\n",
    "try:\n",
    "    from torchvision.models import AlexNet_Weights\n",
    "    alexnet_weights = AlexNet_Weights.DEFAULT\n",
    "except Exception:\n",
    "    alexnet_weights = 'DEFAULT'\n",
    "\n",
    "caffenet = models.alexnet(weights=alexnet_weights)\n",
    "# Trim the last classification layer to get a 4096-D embedding\n",
    "caffenet.classifier = torch.nn.Sequential(*list(caffenet.classifier.children())[:-1])\n",
    "caffenet.eval()\n",
    "caffenet.to(device)\n",
    "\n",
    "# Use the canonical ImageNet preprocessing if available; otherwise fall back to a manual transform\n",
    "try:\n",
    "    transform = alexnet_weights.transforms()\n",
    "except Exception:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "batch_size = 32\n",
    "image_batch = []\n",
    "id_batch = []\n",
    "visual_features = []\n",
    "image_directory = data_path + 'images/'\n",
    "\n",
    "for rid in tqdm(recipe_ids, desc=\"visual_encoding\"):\n",
    "\n",
    "    image_path1 = os.path.join(image_directory, f\"{rid}.jpeg\")\n",
    "    image_path2 = os.path.join(image_directory, f\"{rid}.png\")\n",
    "    image_path3 = os.path.join(image_directory, f\"{rid}.jpg\")\n",
    "    image_path4 = os.path.join(image_directory, f\"{rid}.JPG\")\n",
    "    image_path5 = os.path.join(image_directory, f\"{rid}.PNG\")\n",
    "\n",
    "    image_path = None\n",
    "    for path in [image_path1, image_path2, image_path3, image_path4, image_path5]:\n",
    "        if os.path.exists(path):\n",
    "            image_path = path\n",
    "            break\n",
    "\n",
    "    if not os.path.exists(image_path):\n",
    "        continue\n",
    "    try:\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        img_tensor = transform(img)\n",
    "        image_batch.append(img_tensor)\n",
    "        id_batch.append(rid)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    if len(image_batch) == batch_size:\n",
    "        batch_tensor = torch.stack(image_batch).to(device)\n",
    "        with torch.no_grad():\n",
    "            batch_features = caffenet(batch_tensor).cpu().numpy()  # (N, 4096)\n",
    "        for r_id, feat in zip(id_batch, batch_features):\n",
    "            visual_features.append({'recipe_id': r_id, 'visual_feature': feat})\n",
    "        image_batch, id_batch = [], []\n",
    "\n",
    "# Handle leftover images\n",
    "if len(image_batch) > 0:\n",
    "    batch_tensor = torch.stack(image_batch).to(device)\n",
    "    with torch.no_grad():\n",
    "        batch_features = caffenet(batch_tensor).cpu().numpy()\n",
    "    for r_id, feat in zip(id_batch, batch_features):\n",
    "        visual_features.append({'recipe_id': r_id, 'visual_feature': feat})\n",
    "\n",
    "visual_df = pd.DataFrame(visual_features)\n",
    "\n",
    "# ===========================================\n",
    "# [Textual Feature] (unchanged, per request)\n",
    "# ===========================================\n",
    "detailed_summary_recipe['name'] = detailed_summary_recipe['name'].fillna('')\n",
    "detailed_summary_recipe['ingredients'] = detailed_summary_recipe['ingredients'].fillna('')\n",
    "detailed_summary_recipe['steps'] = detailed_summary_recipe['steps'].fillna('')\n",
    "\n",
    "detailed_summary_recipe['description'] = (\n",
    "    detailed_summary_recipe['name'] + ' ' +\n",
    "    detailed_summary_recipe['ingredients'] + ' ' +\n",
    "    detailed_summary_recipe['steps'] + ' ' +\n",
    "    detailed_summary_recipe['nutrition']\n",
    ")\n",
    "\n",
    "detailed_summary_recipe.drop(columns=['name', 'ingredients', 'nutrition', 'steps'], inplace=True)\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device = device)\n",
    "descriptions = detailed_summary_recipe['description'].tolist()\n",
    "recipe_ids_text = detailed_summary_recipe['id'].tolist()\n",
    "\n",
    "# Encode with batching to avoid OOM\n",
    "text_embeddings = model.encode(descriptions, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "textual_df = pd.DataFrame({\n",
    "    'recipe_id': recipe_ids_text,\n",
    "    'textual_feature': list(text_embeddings)\n",
    "})\n",
    "\n",
    "print(visual_df.head())\n",
    "print(textual_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6561cfb-ce77-40a1-a359-ce1b7e4514b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "print(len(visual_df.iloc[0]['visual_feature']))\n",
    "print(len(textual_df.iloc[0]['textual_feature']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a1817e2-bf86-4235-b6df-99516eea2bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n",
      "image_feat.npy created\n",
      "384\n",
      "text_feat.npy created\n"
     ]
    }
   ],
   "source": [
    "# load user ID mapping & item ID mapping\n",
    "user_id_mapping = pd.read_csv(data_path + 'u_id_mapping.csv', sep='\\t')\n",
    "item_id_mapping = pd.read_csv(data_path + 'i_id_mapping.csv', sep='\\t')\n",
    "\n",
    "visual_dict = dict(zip(visual_df['recipe_id'], visual_df['visual_feature']))\n",
    "textual_dict = dict(zip(textual_df['recipe_id'], textual_df['textual_feature']))\n",
    "\n",
    "# === Create visual feature array ===\n",
    "feat_dim_v = len(next(iter(visual_dict.values())))  # e.g., 4096\n",
    "print(feat_dim_v)\n",
    "visual_feat = np.zeros((len(recipe_ids), feat_dim_v))\n",
    "\n",
    "for idx, row in item_id_mapping.iterrows():\n",
    "    rid = row['recipe_id']\n",
    "    visual_feat[idx] = visual_dict.get(rid, np.zeros(feat_dim_v))  # fallback if missing\n",
    "\n",
    "np.save(data_path + 'image_feat.npy', visual_feat)\n",
    "print('image_feat.npy created')\n",
    "\n",
    "# === Create textual feature array ===\n",
    "feat_dim_t = len(next(iter(textual_dict.values())))  # e.g., 384\n",
    "print(feat_dim_t)\n",
    "textual_feat = np.zeros((len(recipe_ids), feat_dim_t))\n",
    "\n",
    "for idx, row in item_id_mapping.iterrows():\n",
    "    rid = row['recipe_id']\n",
    "    textual_feat[idx] = textual_dict.get(rid, np.zeros(feat_dim_t))  # fallback if missing\n",
    "\n",
    "np.save(data_path + 'text_feat.npy', textual_feat)\n",
    "print('text_feat.npy created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fb5e8f1-f543-4e57-a0e9-dcea33c7ad95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userID\titemID\trating\ttimestamp\tx_label\n",
      "6\t175\t5\t2000-10-23\t0\n",
      "25\t445\t4\t2001-03-24\t0\n",
      "24\t28\t3\t2001-04-02\t0\n",
      "21\t353\t4\t2001-05-07\t0\n",
      "32\t477\t4\t2001-05-16\t0\n",
      "29\t438\t3\t2001-05-21\t0\n",
      "24\t45\t3\t2001-05-25\t0\n",
      "25\t505\t4\t2001-05-30\t0\n",
      "58\t538\t0\t2001-06-16\t0\n",
      "\n",
      "Item ID Mapping:\n",
      "  recipe_id\\titemID\n",
      "0             40\\t0\n",
      "1             49\\t1\n",
      "2             58\\t2\n",
      "3             66\\t3\n",
      "4            142\\t4\n",
      "\n",
      "User ID Mapping:\n",
      "  user_id\\tuserID\n",
      "0         1535\\t0\n",
      "1         1634\\t1\n",
      "2         1676\\t2\n",
      "3         1891\\t3\n",
      "4         2586\\t4\n",
      "Image feature shape: (29905, 4096)\n",
      "[[0.         8.43509769 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "[0.         8.43509769 0.         0.         0.0208737  0.\n",
      " 0.         0.         0.         1.61045408]\n",
      "\n",
      "Text feature shape: (29905, 384)\n",
      "[[-0.08076137 -0.02005294 -0.0355938  ... -0.01922152 -0.04684478\n",
      "  -0.07948707]\n",
      " [-0.0339156  -0.02230442  0.00138124 ...  0.02623551 -0.03224369\n",
      "   0.00162565]\n",
      " [-0.01413294 -0.01359937 -0.05093399 ... -0.00482901 -0.01379578\n",
      "   0.02974985]]\n"
     ]
    }
   ],
   "source": [
    "with open(data_path + 'foodcom.inter', 'r', encoding='utf-8') as f:\n",
    "    for _ in range(10):\n",
    "        print(f.readline().strip())\n",
    "\n",
    "i_mapping = pd.read_csv(data_path + 'i_id_mapping.csv')\n",
    "u_mapping = pd.read_csv(data_path + 'u_id_mapping.csv')\n",
    "\n",
    "print(\"\\nItem ID Mapping:\")\n",
    "print(i_mapping.head())\n",
    "\n",
    "print(\"\\nUser ID Mapping:\")\n",
    "print(u_mapping.head())\n",
    "\n",
    "image_feat = np.load(data_path +'image_feat.npy')\n",
    "text_feat = np.load(data_path +'text_feat.npy')\n",
    "\n",
    "print(\"Image feature shape:\", image_feat.shape)\n",
    "print(image_feat[:3])  # show first 3 entries\n",
    "print(image_feat[0][:10])\n",
    "print(\"\\nText feature shape:\", text_feat.shape)\n",
    "print(text_feat[:3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
